# JPMC_ML_CaseStudy

This repository contains a comprehensive machine learning case study divided into six parts, each focusing on a different core concept in regression, kernel methods, optimization, regularization, and Bayesian learning. The study is performed using synthetic datasets designed to reveal fundamental insights into model behavior, learning dynamics, and generalization.

## **The case study is divided into six parts:**
 
## **Understanding Error Surfaces**:
This section involves generating noisy data based on a height-weight relationship, fitting a linear model using least squares, and analyzing the error surface to estimate and compare model parameters.
 
## **Understanding Model Order and Overfitting**:
Exploring the effects of model complexity and regularization on overfitting by fitting polynomial models of varying degrees, analyzing errors, and examining the impact of regularization on model parameters.
 
## **Understanding the Choice of Kernel**:
Investigating the effectiveness of different kernels (polynomial, Gaussian, and sigmoidal) in fitting data generated from sinusoidal, triangular, and Gaussian functions, analyzing the fit quality across various model orders.
 
## **Understanding Training Parameters**:
Learning the use of stochastic gradient descent for weight updates in kernel-based models, examining the impact of step size and batch size on convergence speed.
 
## **Understanding Bias-Variance Trade-off**:
Generating multiple datasets of noisy sinusoidal data, fit high-order regression models, and analyzing the bias-variance trade-off.
 
## **Exploring Maximum a Posteriori (MAP) Estimation**:
Fitting a high-order regression model to noisy sinusoidal data using Bayesian methods, sampling from the posterior distribution to analyze curve fits, and evaluating the predictive distribution of targets.

